{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neural_network import FNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we will use Fashion MNIST dataset. It's like MNIST number dataset but contains images of fashion accessories instaed of handwritten numbers. We are still looking at a toy dataset, the only reason for using this one over MNIST is becuase MNIST is too old, too boring and too easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and see what is there. We won't to into too much detail about the data, becuase it's not our focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data from CSV\n",
    "train_data = pd.read_csv('./data/fashion-mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0        30        43   \n",
       "3       0    ...            3         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         1         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as comma seprated values (CSV) file, with one row for each image. The first value is the label and next 784 values are the intensity values for each pixel. Each image is $28 \\times 28$ pixel, monochrome image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    6000\n",
       "8    6000\n",
       "7    6000\n",
       "6    6000\n",
       "5    6000\n",
       "4    6000\n",
       "3    6000\n",
       "2    6000\n",
       "1    6000\n",
       "0    6000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have ten different classes, with exactly 6000 images in every class. This is ideal for training, however it's rare to find such ideal data in real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get features (X), which is everything except the column label\n",
    "X_train = train_data[[c for c in train_data.columns if c not in {'label'}]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature value range is 0 to 255\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature value range is\", X_train.min(), \"to\", X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the range for each pixel is 0 to 255. However, we will that see this does not affect us, even if it was different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get label (y), which is the column label in here\n",
    "y_train = train_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to Flot64\n",
    "X_train = np.array(X_train, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that the range does not matter for us is because we are going to scale the features anyway. This is crucial for Feedforward Network or any other classifier based on gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using scikit-learn's StandardScaler. This one will scale each feature individually, which is not crucuial for our case, because all the features come from the same distribution. However, it's necessary when that's not the case. Standard scaler will center all the features, ie their mean will become zero and scale them by the standrd deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize standard scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*fit_transform* will fit the mean and standard deviation as well as scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale using standard scaler\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case test data is processed same as training data, except from scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[c for c in test_data.columns if c not in {'label'}]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.array(X_test, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only use 'tranform', so the scaling will be done using mean and standard deviation from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to the meat of the process. The actual training on the prepared data. We will compare three different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Network Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a baseline Feedforward Network with all the default parameters. This class extends the scikit-learns _BaseEstimator_ class and behaves exactly like most classifier classes in scikit-learn. However, in the backend it uses _Tensorflow (Keras API)_, which eanables fast training using state-of-art algorimns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user is completely abstracted from technical details of the classifier, if they choose to do so. Most importantly, the user does not need to create the _computation graph_. Here, you can see that the classifier is defined without any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_baseline = FNNClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_DeepDummies_ provides a sensible defaults for all necessary parameters. The user does not need to worry about them (except a few,  we will see soon that why it pays off _too worry_ about those few)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the full list of hyperparameters _DeepDummies_ has generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'auto',\n",
       " 'batch_size': 128,\n",
       " 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping at 0x17a8927aba8>],\n",
       " 'class_weight': None,\n",
       " 'dropout': [0.5],\n",
       " 'early_stopping': 5,\n",
       " 'epochs': 100,\n",
       " 'gradient_clipping_norm': None,\n",
       " 'gradient_clipping_value': None,\n",
       " 'hidden_layers': [50],\n",
       " 'l1_penalty': 0.0,\n",
       " 'l2_penalty': 0.0,\n",
       " 'learning_rate': 'auto',\n",
       " 'loss': 'crossentropy',\n",
       " 'metrics': ['accuracy'],\n",
       " 'optimizer': 'adam',\n",
       " 'timeit': True,\n",
       " 'validation_data': None,\n",
       " 'validation_split': 0.1,\n",
       " 'verbosity': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_baseline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's the fun begin! We just need to call _fit_ method like in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (60000, 784) -\t Epochs 100 -\t Batch Size 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden_1 (Dense)             (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "Dropout_1_0.5 (Dropout)      (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "Output_softmax (Dense)       (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 5s 97us/step - loss: 0.7802 - acc: 0.7328 - val_loss: 0.4490 - val_acc: 0.8445\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.5516 - acc: 0.8049 - val_loss: 0.4197 - val_acc: 0.8572\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.5008 - acc: 0.8219 - val_loss: 0.4134 - val_acc: 0.8580\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.4766 - acc: 0.8298 - val_loss: 0.4013 - val_acc: 0.8590\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4653 - acc: 0.8336 - val_loss: 0.3926 - val_acc: 0.8617\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.4505 - acc: 0.8379 - val_loss: 0.3858 - val_acc: 0.8658\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4435 - acc: 0.8404 - val_loss: 0.3862 - val_acc: 0.8627\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.4329 - acc: 0.8436 - val_loss: 0.3896 - val_acc: 0.8627\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 2s 42us/step - loss: 0.4249 - acc: 0.8468 - val_loss: 0.3764 - val_acc: 0.8690\n",
      "Epoch 10/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4222 - acc: 0.8479 - val_loss: 0.3738 - val_acc: 0.8702\n",
      "Epoch 11/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4172 - acc: 0.8479 - val_loss: 0.3770 - val_acc: 0.8668\n",
      "Epoch 12/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4148 - acc: 0.8508 - val_loss: 0.3722 - val_acc: 0.8708\n",
      "Epoch 13/100\n",
      "54000/54000 [==============================] - 2s 42us/step - loss: 0.4068 - acc: 0.8515 - val_loss: 0.3700 - val_acc: 0.8703\n",
      "Epoch 14/100\n",
      "54000/54000 [==============================] - 2s 42us/step - loss: 0.4040 - acc: 0.8552 - val_loss: 0.3672 - val_acc: 0.8725\n",
      "Epoch 15/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4011 - acc: 0.8560 - val_loss: 0.3663 - val_acc: 0.8723\n",
      "Epoch 16/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.4008 - acc: 0.8532 - val_loss: 0.3673 - val_acc: 0.8733\n",
      "Epoch 17/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.3952 - acc: 0.8566 - val_loss: 0.3672 - val_acc: 0.8723\n",
      "Epoch 18/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.3916 - acc: 0.8569 - val_loss: 0.3704 - val_acc: 0.8708\n",
      "Epoch 19/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.3906 - acc: 0.8575 - val_loss: 0.3753 - val_acc: 0.8692\n",
      "Fit complete in 48.03 seconds\n"
     ]
    }
   ],
   "source": [
    "fnn_baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy and loss on testing data. We can use _score_ method, which will return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8739, 'loss': 0.35596662336587903}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_baseline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also predict the labels and get the accuracy with scikit-learn's _accuracy_score_ method. As said before, this class is designed to be fully compatible with scikit-learn's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8739"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(fnn_baseline.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Network Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said before that, it pays off to 'tweak' some hyperparameters. And _DeepDummies_ gives you freedom to tweak most of them, at the same time keep it simple and providing sensible defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with them. One of the most \"unreasonable\" default in the previous classifier was the number of units in the hidden layer set to 50. With 784 features it is worth increasing that number. So let's change it to 250. Notice that we provide a list as value. The number of elements in the list corresponds to the number of hidden layers. For eg [250, 100] will create graph with 2 hidden layers, first with 250 units and second with 100 units. More details can be found in _API Reference_ document. \n",
    "\n",
    "We are also increasing dropout a bit, to avoid overfitting due to more units. This hyperparameters were set by intution followed by brief trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fnn = FNNClassifier(hidden_layers=[250], \n",
    "                    dropout=0.6,\n",
    "                    early_stopping=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'auto',\n",
       " 'batch_size': 128,\n",
       " 'callbacks': [<tensorflow.python.keras.callbacks.EarlyStopping at 0x17a88a15320>],\n",
       " 'class_weight': None,\n",
       " 'dropout': [0.6],\n",
       " 'early_stopping': 10,\n",
       " 'epochs': 100,\n",
       " 'gradient_clipping_norm': None,\n",
       " 'gradient_clipping_value': None,\n",
       " 'hidden_layers': [250],\n",
       " 'l1_penalty': 0.0,\n",
       " 'l2_penalty': 0.0,\n",
       " 'learning_rate': 'auto',\n",
       " 'loss': 'crossentropy',\n",
       " 'metrics': ['accuracy'],\n",
       " 'optimizer': 'adam',\n",
       " 'timeit': True,\n",
       " 'validation_data': None,\n",
       " 'validation_split': 0.1,\n",
       " 'verbosity': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size (60000, 784) -\t Epochs 100 -\t Batch Size 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden_1 (Dense)             (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "Dropout_1_0.6 (Dropout)      (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "Output_softmax (Dense)       (None, 10)                2510      \n",
      "=================================================================\n",
      "Total params: 198,760\n",
      "Trainable params: 198,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "54000/54000 [==============================] - 3s 58us/step - loss: 0.6417 - acc: 0.7865 - val_loss: 0.4225 - val_acc: 0.8552\n",
      "Epoch 2/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.4576 - acc: 0.8389 - val_loss: 0.3854 - val_acc: 0.8622\n",
      "Epoch 3/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.4174 - acc: 0.8514 - val_loss: 0.3710 - val_acc: 0.8693\n",
      "Epoch 4/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.3911 - acc: 0.8586 - val_loss: 0.3613 - val_acc: 0.8747\n",
      "Epoch 5/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.3778 - acc: 0.8622 - val_loss: 0.3512 - val_acc: 0.8800\n",
      "Epoch 6/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3664 - acc: 0.8673 - val_loss: 0.3468 - val_acc: 0.8782\n",
      "Epoch 7/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3539 - acc: 0.8712 - val_loss: 0.3407 - val_acc: 0.8822\n",
      "Epoch 8/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3468 - acc: 0.8737 - val_loss: 0.3399 - val_acc: 0.8840\n",
      "Epoch 9/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3433 - acc: 0.8747 - val_loss: 0.3491 - val_acc: 0.8810\n",
      "Epoch 10/100\n",
      "54000/54000 [==============================] - 2s 46us/step - loss: 0.3352 - acc: 0.8771 - val_loss: 0.3438 - val_acc: 0.8800\n",
      "Epoch 11/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.3287 - acc: 0.8809 - val_loss: 0.3303 - val_acc: 0.8875\n",
      "Epoch 12/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.3243 - acc: 0.8821 - val_loss: 0.3278 - val_acc: 0.8890\n",
      "Epoch 13/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3188 - acc: 0.8835 - val_loss: 0.3323 - val_acc: 0.8885\n",
      "Epoch 14/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.3122 - acc: 0.8859 - val_loss: 0.3305 - val_acc: 0.8902\n",
      "Epoch 15/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3104 - acc: 0.8862 - val_loss: 0.3379 - val_acc: 0.8847\n",
      "Epoch 16/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.3057 - acc: 0.8884 - val_loss: 0.3244 - val_acc: 0.8848\n",
      "Epoch 17/100\n",
      "54000/54000 [==============================] - 2s 46us/step - loss: 0.2998 - acc: 0.8902 - val_loss: 0.3231 - val_acc: 0.8933\n",
      "Epoch 18/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.2992 - acc: 0.8902 - val_loss: 0.3308 - val_acc: 0.8890\n",
      "Epoch 19/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.2929 - acc: 0.8918 - val_loss: 0.3222 - val_acc: 0.8932\n",
      "Epoch 20/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.2886 - acc: 0.8937 - val_loss: 0.3269 - val_acc: 0.8893\n",
      "Epoch 21/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.2856 - acc: 0.8951 - val_loss: 0.3359 - val_acc: 0.8858\n",
      "Epoch 22/100\n",
      "54000/54000 [==============================] - 2s 43us/step - loss: 0.2847 - acc: 0.8943 - val_loss: 0.3386 - val_acc: 0.8862\n",
      "Epoch 23/100\n",
      "54000/54000 [==============================] - 2s 46us/step - loss: 0.2782 - acc: 0.8966 - val_loss: 0.3324 - val_acc: 0.8868\n",
      "Epoch 24/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.2741 - acc: 0.8986 - val_loss: 0.3307 - val_acc: 0.8912\n",
      "Epoch 25/100\n",
      "54000/54000 [==============================] - 2s 46us/step - loss: 0.2750 - acc: 0.9001 - val_loss: 0.3237 - val_acc: 0.8962\n",
      "Epoch 26/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.2683 - acc: 0.9022 - val_loss: 0.3275 - val_acc: 0.8925\n",
      "Epoch 27/100\n",
      "54000/54000 [==============================] - 2s 45us/step - loss: 0.2672 - acc: 0.9021 - val_loss: 0.3369 - val_acc: 0.8928\n",
      "Epoch 28/100\n",
      "54000/54000 [==============================] - 2s 44us/step - loss: 0.2653 - acc: 0.9018 - val_loss: 0.3279 - val_acc: 0.8937\n",
      "Fit complete in 68.43 seconds\n"
     ]
    }
   ],
   "source": [
    "fnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8958, 'loss': 0.31569191516041756}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8958"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(fnn.predict_proba(X_test), axis=1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Voilà!_ With small tweaking we got about 2.5% improvement in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare it with scikit-learn's SGDClassifier. You can think of this one as an FNN without hidden layer (FNN as SGD Classifier with hidden layers). Roughly speaking, this is generalized version of Logistic Regression which supports multiway classification. (again, roughly and practically speaking, technically it's very different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take time to compare this one with the two classifier used above. You will see they are almost identical for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=1000, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'class_weight': None,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.0,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'optimal',\n",
       " 'loss': 'hinge',\n",
       " 'max_iter': 1000,\n",
       " 'n_iter': None,\n",
       " 'n_jobs': 1,\n",
       " 'penalty': 'l2',\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=1000, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8372"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sgd.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the results are not identical. With the hidden layer we got much better accuracy with same human effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
